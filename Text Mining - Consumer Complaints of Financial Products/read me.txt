我目前做的关于Naive Bayes的内容都在这里了，模型的performance如下：

我发现：
1. 对文本进行tokenization + stopwords(以及"XX" "XXXX") removal + punctuation removal时模型的表现最好。
  - 初始是只做tokenization，且模型的参数均为默认值时，tf-idf shape=(87952,53779);
  - 我后面先后尝试了去stopword+标点，去non-English words，token stemming，token lemmatizing的几种组合，比较发现还是只去stopword+标点得到的结果最好。

2. 我调的参数一个是TfidfVectorizer()的min_df，另一个是MultinomialNB()的alpha，根据grid search的结果，选择min_df=10，alpha=0.1。这对token个数也有影响，比如前面提到的初始tokenization，调参后tf-idf shape就变成(87952,12697)。但min_df继续增大，alpha继续减小，还会提高模型的准确度。

3. 在model validation时single fold(我是按train:test=7:3做的）比K-fold cross validation得到的model performance更好。

4. 随着sample size增加（按璐璐给的8个dataset依次导入），fscore的均值大体呈增加趋势，fscore的方差大体呈降低趋势，但也有波动。